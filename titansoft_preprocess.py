# -*- coding: utf-8 -*-
"""Titansoft Deep SAD.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O98RFyve5x2YW3tGEVLApWgiyRpjF3PV
"""

# !pip install category_encoders

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer
from sklearn.svm import OneClassSVM, SVC

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import SimpleImputer,IterativeImputer
from sklearn.metrics import confusion_matrix, accuracy_score,f1_score, precision_score
import matplotlib.pyplot as plt

import pandas as pd

# upload data
# Load the training data
train = pd.read_csv("/content/Titansoft DS train.csv_2021.csv")
test = pd.read_csv("/content/Titansoft DS test.csv_2021.csv")

"""# TODO
* explor data
* impute fill nan X usless
* standerlize X usless
* class to number V
* split V
* xgtboost V
* Deep SAD
* kfild V 
* label imblance 
  * 20% soso maybe work
  * Oversampling 
    * SMOTE
  * Undersampling 
    * TomekLinks X
* feature selection
  * remove billamont mouth V good
* stacking 
* Pseudo Labeling 
* Hyper-parameter tuning with scikit-optimize
* Anomaly dectation
  * one class svmX bad
  * Isolat forest
* read :Tabular Data: Deep Learning is Not All You Need
  * xgboost first 
  * deep learning if time avaliable


* what important?
  * miss detation?
  *

# Explor data
"""

train

test

# print(train.describe())
# print(test.describe())
print(len(train))
print(len(test))

# id
train_id = train['id']
test_id = test['id']
train = train.drop(['id'],axis=1)
test = test.drop(['id'],axis=1)

"""# data preprocess"""

train = train.replace('unknown', np.nan)
train = train.replace('0', np.nan)
train = train.drop(['bill_amount_month4','bill_amount_month3','bill_amount_month2','bill_amount_month1'],axis=1)
# train = train.drop(['payment_delay_month3(larger value means longer delay)',
#             'payment_delay_month2(larger value means longer delay)',
#             'payment_delay_month1(larger value means longer delay)'],axis=1)
# train = train.drop(['payment_delay_month4(larger value means longer delay)'],axis=1)

# remove <18 
# train = train[train['age']>18]

# rm 4clos = 21329
# dropna
train_dna = train.dropna() # 22780-20262 = 2518 GG #baseline   replace '0' 'unknow => 19978 
# train_dna = train.dropna(subset=class_col)

train_dna

# split 
y = train_dna['label_month5_payment']
features = train_dna.drop(['label_month5_payment'], axis=1)

# one hoteencoding 
categoricals = ['gender', 'education_level', 'marital_status']
categoricals

features_cols = features[categoricals]

enc = OneHotEncoder(drop='first', sparse=False)
features_cols_enc = pd.DataFrame(enc.fit_transform(features_cols))
features_cols_enc 

# LeaveOneOut
# ce_leave = ce.LeaveOneOutEncoder(cols=categoricals,sigma = 0.05)
# features_cols_leave = ce_leave.fit_transform(features_cols, y)

# features_cols = features_cols_leave
features_cols = features_cols_enc

features_cols

features_cols = features_cols.reset_index(drop=True)

# stander
features_num = features.drop(categoricals, axis=1).reset_index(drop=True)

features_num_cols = features_num.columns
#imput age
features_num['age'] = features_num['age'].replace(0, np.nan)
features_num['age'] = features_num['age'].replace(255, np.nan)

features_num[features_num['age'].isnull()]

## normalizer
# normalizer = Normalizer()
# features_num =  normalizer.fit_transform(features_num)
# scaler = StandardScaler()
# features_num = pd.DataFrame(scaler.fit_transform(features_num))

## imputation  useless
# imputer = SimpleImputer()
# features_num = pd.DataFrame(imputer.fit_transform(features_num))

imputer = IterativeImputer(max_iter=10, random_state=0)
features_num = pd.DataFrame(imputer.fit_transform(features_num), columns =features_num_cols)

#14~
n_col_name = features_num.columns
features_num

features_cols = features_cols.reset_index(drop=True)
features_num = features_num.reset_index(drop=True)

df = pd.concat([features_cols, features_num],axis=1)
len_col = len(df.columns)
df.columns =[i for i in range(0,len_col)]

features_cols



y = y.reset_index(drop=True)

X_train, X_valid, y_train, y_valid = train_test_split(df, y, train_size=0.8, random_state=0)

print('y_train 1 :',y_train.where(y_train==1).count()/len(y_train))
print('y_train 0 :',y_train.where(y_train==0).count()/len(y_train))

print('y_valid 1 :', y_valid.where(y_valid==1).count()/len(y_valid))
print('y_valid 0 :', y_valid.where(y_valid==0).count()/len(y_valid))

X_train

y_train

# x_1 = [[1,2],[4,5],[7,8],[10,11],[13,14]]
# y_1 = [3,6,9,12,15]
# X_train, X_valid, y_train, y_valid =train_test_split(x_1, y_1, train_size=0.8, random_state=0)

# X_train = X_train.reset_index()
# y_train = y_train.reset_index()
# X_valid = y_train.reset_index()
# y_valid = y_valid.reset_index()

train_df = pd.merge(X_train, y_train, left_index=True, right_index=True)
valid_df = pd.merge(X_valid, y_valid, left_index=True, right_index=True)

train_df

valid_df

train_df.to_csv("train_80.csv", index=False)
valid_df.to_csv("valid_80.csv", index=False)

df  = pd.read_csv("train_80.csv")

df

"""#training

# eval accuracy 
## lgbt
* default  0.8519615099925981 
*   0.8517147791759191 ***
*
```
early_stopping_rounds = 20
objective = 'binary', learning_rate = 0.05, 
n_estimators = 100, random_state=0)
```
*  0.8507278559092031
```
stander same previous
```

* loveonout unimput : 0.8529484332593141
* loveonout imput : 0.8497642039074781
* onehot unimput : 0.8516016016016016
* onehot unimput rm4cols rm0unknow :  *
  * acc 0.8527894983591187
  * f1 0.42700729927007297
* onehot unimput rm4cols rm0unknow SMOTE: 0.8490389123300516
* onehot imput rm4cols rm0unknow:: 0.8396720564791619
* Kfold-5 onehot unimput rm4cols rm0unknow : 0.8530239099859352 **
* Kfold-5 onehot unimput rm4cols rm0unknow TomekLinks : 0.8393351800554016
* Kfold-5 onehot unimput rm4cols rm0unknow SMOTE : 0.8478668541959681
* Kfold-10 onehot unimput rm4cols rm0unknow :
  * acc 0.8530239099859352
  * f1 0.42529789184234645

* Kfold-10 onehot unimput rm4cols rm0unknow rm<18:
  * acc 0.8525052928722654
  * f1 0.4199814986123959
* Kfold-10 onehot unimput rm4cols rm0unknow imputage<18:
  * acc 0.8534927332395686 ***
  * f1 0.42502299908003677

## xgb
### split 0.8 
* ipr : 0.8504379070289693
* default : 0.8534418948926721
```
early_stopping_rounds=10 ,random_state=0
```
* default imput: 0.8504379070289693
* default onthot rm 0 unknow: 0.8543543543543544 ***
* default onehot unimput rm 0 unknow rm4cols:
  *acc 0.8541959681200187 ***
  *f1 0.43144424131627057
* default onehot unimput rm 0 unknow rm4cols rm age<18:
  * acc 0.852975770406963
  * f1 0.42817932296431843
* default onehot  rm 0 unknow rm4cols rm imputage<18:
  * acc 0.8541959681200187
  * f1 0.43144424131627057
* default onehot imput rm 0 unknow rm4cols: 0.8394443179230243
* default onehot imput rm 0 unknow rm4cols Tomek Links: 0.8390833543188114
* default onehot imput rm 0 unknow rm4cols SMOT:0.8499765588373184
* default loveonout unimput: 0.8539353565260301
  * 0.8541820873427091
* default loveonout imput: 0.850662474736133
* default loveonout unimput rm 0 unknow:
  * 0.8543543543543544 ***
* Kfold-5 : 0.8534418948926721 
* Kfold-5 std nor: 0.8344436220083888
* Kfold-5 imput 0.850662474736133
* Kfold-5 loveonout imput:0.850662474736133
* Kfold-5 loveonout unimput rm 0 unknow:0.8538538538538538
* Kfold-5 one hot unimput rm 0 unknow:0.8533533533533534
* Kfold-5 one hot unimput rm 0 unknow rm4cols:0.8544303797468354 ***
* Kfold-5 one hot imput rm 0 unknow rm4cols: 0.8383056251423366
* Kfold-5 one hot imput rm 0 unknow rm4cols SMOT: 
  * acc 0.8490389123300516  
* Kfold-5 one hot imput rm 0 unknow rm4cols NN: 
  * acc 0.8370839193624003 
  * f1 0.4794007490636704
* Kfold-10 loveonout unimpu: 0.8536886257093511
  * 0.8536886257093511
* Kfold-10 one hot unimput rm 0: 0.8541041041041041
* Kfold-10 loveonout unimput rm 0 unknow rm4cols: 
  * acc 0.8541959681200187
  * f1 0.43040293040293043
* Kfold-10 onehot  rm 0 unknow rm4cols rm imputage<18:
  
  * 0.8541959681200187
  * 0.43040293040293043


## one class svn
"""